{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1: Text Suggestion\n",
    "\n",
    "__Мягкий дедлайн: 24.09 23:59__   \n",
    "__Жесткий дедлайн: 27.09 23:59__\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать систему, предлагающую удачное продолжение слова или нескольких следующих слов в режиме реального времени по типу тех, которые используются в почте или поисковой строке. За дополнительные баллы полученную систему нужно будет обернуть в пользовательский интерфейс с помощью библиотеки [reflex](https://github.com/reflex-dev/reflex) или аналогов. В этой домашке вам не придется обучать никаких моделей, мы ограничимся n-граммной генерацией.\n",
    "\n",
    "### Структура\n",
    "\n",
    "Это домашнее задание состоит из двух частей: основной и бонусной. В первой вам нужно будет выполнить 5 заданий, по итогам которых вы получите минимально рабочее решение. А во второй, пользуясь тем, что вы уже сделали, реализовать полноценную систему подсказки текста с пользовательским интерфейсом. Во второй части мы никак не будем ограничивать вашу фантазию. Делайте что угодно, лишь бы в результате получился удобный фреймворк. Чем лучше у вас будет результат, тем больше баллов вы получите. Если будет совсем хорошо, то мы добавим бонусов сверху по своему усмотрению.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 15 баллов. Сдавать задание после жесткого дедлайна нельзя. При сдачи решения после мягкого дедлайна за каждый день просрочки снимается по __одному__ баллу.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код.\n",
    "\n",
    "При сдаче зададания в anytask вам будет необходимо сдать весь код, а если вы возьметесь за бонусную часть, то еще отчет и видео с демонстрацией вашего UI. За основную часть можно получить до __10-ти__ баллов, а за бонусную – до __5-ти__ баллов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T19:36:40.823929600Z",
     "start_time": "2025-09-21T19:36:38.452336300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: reflex in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.8.11)\n",
      "Requirement already satisfied: validators in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.35.0)\n",
      "Requirement already satisfied: jupyter in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: ipywidgets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (8.1.7)\n",
      "Requirement already satisfied: nbmultitask in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.1.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: alembic<2.0,>=1.15.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (1.16.5)\n",
      "Requirement already satisfied: granian>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from granian[reload]>=2.4.0->reflex) (2.5.4)\n",
      "Requirement already satisfied: httpx<1.0,>=0.23.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (0.28.1)\n",
      "Requirement already satisfied: packaging<26,>=24.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (25.0)\n",
      "Requirement already satisfied: platformdirs<5.0,>=4.3.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (4.4.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.10.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (2.11.9)\n",
      "Requirement already satisfied: python-multipart<1.0,>=0.0.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (0.0.20)\n",
      "Requirement already satisfied: python-socketio<6.0,>=5.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (5.13.0)\n",
      "Requirement already satisfied: redis<7.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (6.4.0)\n",
      "Requirement already satisfied: reflex-hosting-cli>=0.1.55 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (0.1.55)\n",
      "Requirement already satisfied: rich<15,>=13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (14.1.0)\n",
      "Requirement already satisfied: sqlmodel<0.1,>=0.0.24 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (0.0.25)\n",
      "Requirement already satisfied: starlette>=0.47.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (0.48.0)\n",
      "Requirement already satisfied: typing-extensions>=4.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (4.15.0)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from reflex) (1.17.3)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from alembic<2.0,>=1.15.2->reflex) (2.0.43)\n",
      "Requirement already satisfied: Mako in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from alembic<2.0,>=1.15.2->reflex) (1.3.10)\n",
      "Requirement already satisfied: tomli in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from alembic<2.0,>=1.15.2->reflex) (2.2.1)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1.0,>=0.23.3->reflex) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1.0,>=0.23.3->reflex) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1.0,>=0.23.3->reflex) (1.0.9)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1.0,>=0.23.3->reflex) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0,>=0.23.3->reflex) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3.0,>=1.10.21->reflex) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3.0,>=1.10.21->reflex) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3.0,>=1.10.21->reflex) (0.4.1)\n",
      "Requirement already satisfied: bidict>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-socketio<6.0,>=5.12.0->reflex) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-socketio<6.0,>=5.12.0->reflex) (4.12.2)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from redis<7.0,>=5.2.1->reflex) (5.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich<15,>=13->reflex) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich<15,>=13->reflex) (2.19.2)\n",
      "Requirement already satisfied: notebook in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (7.4.5)\n",
      "Requirement already satisfied: jupyter-console in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from jupyter) (6.30.1)\n",
      "Requirement already satisfied: jupyterlab in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter) (4.4.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: watchfiles~=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from granian[reload]>=2.4.0->reflex) (1.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio->httpx<1.0,>=0.23.3->reflex) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio->httpx<1.0,>=0.23.3->reflex) (1.3.1)\n",
      "Requirement already satisfied: decorator in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: stack_data in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15,>=13->reflex) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-engineio>=4.11.0->python-socketio<6.0,>=5.12.0->reflex) (1.1.0)\n",
      "Requirement already satisfied: wsproto in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio<6.0,>=5.12.0->reflex) (1.2.0)\n",
      "Requirement already satisfied: appnope>=0.1.2 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (0.1.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (1.8.16)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (5.8.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from ipykernel->jupyter) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ipykernel->jupyter) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-client>=8.0.0->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->jupyter) (1.17.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.0.5)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab->jupyter) (65.5.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.23.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.25.1)\n",
      "Requirement already satisfied: requests>=2.31 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.5)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2>=3.0.3->jupyterlab->jupyter) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.27.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: fqdn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.1.0)\n",
      "Requirement already satisfied: uri-template in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (4.13.5)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.21.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.5.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.2.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.23)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.8)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20250822)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/lena-grish/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk reflex validators jupyter ipywidgets nbmultitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T19:36:43.348626900Z",
     "start_time": "2025-09-21T19:36:40.823929600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lena-\n",
      "[nltk_data]     grish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from urllib.parse import urlparse\n",
    "from typing import List, Union, Optional, Tuple\n",
    "import numpy as np\n",
    "import reflex as rx\n",
    "from tqdm.notebook import tqdm\n",
    "import validators\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import nbmultitask\n",
    "import joblib\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "import pickle\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "Для получения текстовых статистик используйте датасет `emails.csv`. Вы можете найти его по [ссылке](https://disk.yandex.ru/d/ikyUhWPlvfXxCg). Он содержит более 500 тысяч электронных писем на английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T19:36:54.956724600Z",
     "start_time": "2025-09-21T19:36:43.350629300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517401"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = pd.read_csv(\"emails.csv\")\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T19:36:54.957734200Z",
     "start_time": "2025-09-21T19:36:54.933446200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517396</th>\n",
       "      <td>zufferli-j/sent_items/95.</td>\n",
       "      <td>Message-ID: &lt;26807948.1075842029936.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517397</th>\n",
       "      <td>zufferli-j/sent_items/96.</td>\n",
       "      <td>Message-ID: &lt;25835861.1075842029959.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517398</th>\n",
       "      <td>zufferli-j/sent_items/97.</td>\n",
       "      <td>Message-ID: &lt;28979867.1075842029988.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517399</th>\n",
       "      <td>zufferli-j/sent_items/98.</td>\n",
       "      <td>Message-ID: &lt;22052556.1075842030013.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517400</th>\n",
       "      <td>zufferli-j/sent_items/99.</td>\n",
       "      <td>Message-ID: &lt;28618979.1075842030037.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517401 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file  \\\n",
       "0           allen-p/_sent_mail/1.   \n",
       "1          allen-p/_sent_mail/10.   \n",
       "2         allen-p/_sent_mail/100.   \n",
       "3        allen-p/_sent_mail/1000.   \n",
       "4        allen-p/_sent_mail/1001.   \n",
       "...                           ...   \n",
       "517396  zufferli-j/sent_items/95.   \n",
       "517397  zufferli-j/sent_items/96.   \n",
       "517398  zufferli-j/sent_items/97.   \n",
       "517399  zufferli-j/sent_items/98.   \n",
       "517400  zufferli-j/sent_items/99.   \n",
       "\n",
       "                                                  message  \n",
       "0       Message-ID: <18782981.1075855378110.JavaMail.e...  \n",
       "1       Message-ID: <15464986.1075855378456.JavaMail.e...  \n",
       "2       Message-ID: <24216240.1075855687451.JavaMail.e...  \n",
       "3       Message-ID: <13505866.1075863688222.JavaMail.e...  \n",
       "4       Message-ID: <30922949.1075863688243.JavaMail.e...  \n",
       "...                                                   ...  \n",
       "517396  Message-ID: <26807948.1075842029936.JavaMail.e...  \n",
       "517397  Message-ID: <25835861.1075842029959.JavaMail.e...  \n",
       "517398  Message-ID: <28979867.1075842029988.JavaMail.e...  \n",
       "517399  Message-ID: <22052556.1075842030013.JavaMail.e...  \n",
       "517400  Message-ID: <28618979.1075842030037.JavaMail.e...  \n",
       "\n",
       "[517401 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что данные очень грязные. В каждом письме содержится различная мета-информация, которая будет только мешать при предсказании продолжения текста.\n",
    "\n",
    "__Задание 1 (2 балла).__ Очистите корпус текстов по вашему усмотрению и объясните свой выбор. В идеале обработанные тексты должны содержать только текст самого письма и ничего лишнего по типу ссылок, адресатов и прочих символов, которыми мы точно не хотим продолжать текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Моя логика:\n",
    "> - Убираем инфу о письме (когда, кому, от кого и прочее) - контролирую это набором _плохих_ начал `trash_starts`\n",
    "> - Все приводим к нижнему регистру\n",
    "> - Удаляем числа (`if word.isnumeric()`) и ссылки (`if urlparse(word).scheme`)\n",
    "> - Чистим пунктуацию и делим на токены: `reg.findall(text)`\n",
    "> - А еще хочу, чтобы у слов был контекст, поэтому < 3 слов в строке не хочу\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T19:36:54.957734200Z",
     "start_time": "2025-09-21T19:36:54.947531200Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T19:36:54.957734200Z",
     "start_time": "2025-09-21T19:36:54.949056700Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\") + ['dd', 'mm', 'yy', 'yyyy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T19:36:54.996793300Z",
     "start_time": "2025-09-21T19:36:54.953198Z"
    }
   },
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    def is_ok_word(word: str):\n",
    "        if word.isnumeric() or re.search(r\"\\d\", word):\n",
    "            return False\n",
    "        if \"@\" in word:\n",
    "            return False\n",
    "        if word in stop_words:\n",
    "            return False\n",
    "        if (\n",
    "            \"www\" in word\n",
    "            or word.startswith((\"http:\\\\\", \"https:\\\\\"))\n",
    "            or word.endswith((\".com\", \".org\", \".net\"))\n",
    "            or validators.url(word)\n",
    "        ):\n",
    "            return False\n",
    "        if len(word) < 2:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    text = text.split(\"\\n\")\n",
    "    clear_text = []\n",
    "\n",
    "    trash_starts = (\n",
    "        \"message-id:\",\n",
    "        \"date:\",\n",
    "        \"from:\",\n",
    "        \"to:\",\n",
    "        \"subject:\",\n",
    "        \"mime-version:\",\n",
    "        \"content-type:\",\n",
    "        \"content-transfer-encoding:\",\n",
    "        \"x-from:\",\n",
    "        \"x-to:\",\n",
    "        \"x-cc:\",\n",
    "        \"x-bcc:\",\n",
    "        \"x-folder:\",\n",
    "        \"x-origin:\",\n",
    "        \"x-filename:\",\n",
    "        \"full name:\",\n",
    "        \"login \",\n",
    "        \"extension:\",\n",
    "        \"office location:\",\n",
    "        'static ip address',\n",
    "        \"cc\",\n",
    "        \"---\",\n",
    "        \"***\",\n",
    "    )\n",
    "\n",
    "    for row in text:\n",
    "        row = row.strip().lower()\n",
    "        if row.startswith(trash_starts) or len(row) == 0:\n",
    "            continue\n",
    "\n",
    "        row = \" \".join([x for x in row.split() if is_ok_word(x)])\n",
    "        reg = re.compile(r\"\\w+\")\n",
    "        row = reg.findall(row)\n",
    "        row = [x for x in row if is_ok_word(x)]\n",
    "        if len(row) < 3:\n",
    "            continue\n",
    "        clear_text.append(\" \".join(row))\n",
    "    return \"\\n\".join(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-21T19:36:54.957734200Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a580a8274be9489f90e1b4a903b207c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clear_texts = [clear_text(text) for text in tqdm(emails[\"message\"][:100])]\n",
    "clear_texts = [text for text in clear_texts if len(text.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enclosed demographics westgate site investor alliance\n",
      "investor alliance says demographics similar package\n",
      "san marcos received earlier\n",
      "questions information requirements let know\n",
      "let know interest level westgate project\n",
      "property across street sagewood units san marcos\n",
      "sale approved units land selling per square\n",
      "foot one two remaining approved multifamily parcels west\n",
      "san marcos moratorium development\n",
      "several new studies looked show rents duplexes\n",
      "new units going significantly higher roughly\n",
      "per square foot leased entire unit lease\n",
      "psf leased term individual room\n",
      "property best location student housing new\n",
      "project serious interest please let know\n",
      "short window opportunity equity requirement\n",
      "yet known would likely secure land\n",
      "know question later today\n",
      "president creekside builders llc\n",
      "____________________________________________________________________________________________________\n",
      "meeting tuesday oct regarding\n",
      "storage strategies west please mark calendars\n",
      "ena denver office\n",
      "____________________________________________________________________________________________________\n",
      "please use second check october payment already\n",
      "tossed let know mail another\n",
      "____________________________________________________________________________________________________\n",
      "think fletch good cpa still\n",
      "____________________________________________________________________________________________________\n",
      "please use second check october payment copy\n",
      "original deal want fax\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(15, 20):\n",
    "    print(clear_texts[i])\n",
    "    print(\"_\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для следующего задания вам нужно будет токенизировать текст. Для этого просто разбейте его по словам. Очевидно, итоговый результат для финального пользователя будет лучше, если ваша система также будет предлагать уместную пунктуацию. Но если вы заметите, что из-за этого падает качество самого текса, то можете удалить все небуквенные символы на этапе токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "clear_texts = [text.split() for text in clear_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая схема решения\n",
    "\n",
    "Мы хотим сделать систему, которая будет ускорять набор текста, советуя подходящие продолжения. Для подсказки следующего слова мы будем использовать n-граммную модель. Так как n-граммная модель работает с целыми словами, а советы мы хотим давать в риал-тайме даже когда слово еще не дописано, сперва надо научиться дополнять слово до целого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение слова\n",
    "\n",
    "В этой части вам предстоит реализовать метод дополнения слова до целого по его началу (префиксу). Для этого сперва необходимо научиться находить все слова, имеющие определенный префикс. Мы будем вызывать функцию поиска подходящих слов после каждой напечатанной пользователем буквы. Поэтому нам очень важно, чтобы поиск работал как можно быстрее. Простой перебор всех слов занимает $O(|V| \\cdot n)$ времени, где $|V|$ – размер словаря, а $n$ – длина префикса. Мы же напишем [префиксное дерево](https://ru.wikipedia.org/wiki/Префиксное_дерево), которое позволяет искать слова не больше чем за $O(n + mk)$, где $m$ - число подходящих слов, а $k$ – длина суффикса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2 (2 балла).__ Допишите префиксное дерево для поиска слов по префиксу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class PrefixTreeNode:\n",
    "    def __init__(self, word: str = \"\"):\n",
    "        self.children: dict[str, PrefixTreeNode] = {}\n",
    "        self.word = word\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "    def get_all_children(self):\n",
    "        all_children = []\n",
    "        if self.is_end_of_word:\n",
    "            all_children.append(self)\n",
    "        for char in self.children:\n",
    "            child = self.children[char]\n",
    "            if child.is_end_of_word:\n",
    "                all_children.append(child)\n",
    "            all_children.extend(child.get_all_children())\n",
    "        return all_children\n",
    "\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self, vocabulary: List[str]):\n",
    "        \"\"\"\n",
    "        vocabulary: список всех уникальных токенов в корпусе\n",
    "        \"\"\"\n",
    "        self.root = PrefixTreeNode()\n",
    "\n",
    "        # your code here\n",
    "        for word in vocabulary:\n",
    "            node = self.root\n",
    "            for char in word:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = PrefixTreeNode(node.word + char)\n",
    "                node = node.children[char]\n",
    "            node.is_end_of_word = True\n",
    "\n",
    "    def search_prefix(self, prefix) -> List[str]:\n",
    "        \"\"\"\n",
    "        Возвращает все слова, начинающиеся на prefix\n",
    "        prefix: str – префикс слова\n",
    "        \"\"\"\n",
    "\n",
    "        # your code here\n",
    "        node = self.root\n",
    "        for char in prefix:\n",
    "            if char not in node.children:\n",
    "                return []\n",
    "            node = node.children[char]\n",
    "        children = node.get_all_children()\n",
    "        children_words = [child.word for child in children]\n",
    "\n",
    "        return list(set(children_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "vocabulary = [\"aa\", \"aaa\", \"abb\", \"bba\", \"bbb\", \"bcd\"]\n",
    "prefix_tree = PrefixTree(vocabulary)\n",
    "\n",
    "assert set(prefix_tree.search_prefix(\"a\")) == set([\"aa\", \"aaa\", \"abb\"])\n",
    "assert set(prefix_tree.search_prefix(\"bb\")) == set([\"bba\", \"bbb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть способ быстро находить все слова с определенным префиксом, нам нужно их упорядочить по вероятности, чтобы выбирать лучшее. Будем оценивать вероятность слова по частоте его __встречаемости в корпусе__.\n",
    "\n",
    "__Задание 3 (2 балла).__ Допишите класс `WordCompletor`, который формирует словарь и префиксное дерево, а так же умеет находить все возможные продолжения слова вместе с их вероятностями. В этом классе вы можете при необходимости дополнительно отфильтровать слова, например, удалив все самые редкие. Постарайтесь максимально оптимизировать ваш код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class WordCompletor:\n",
    "    def __init__(self, corpus, border=0.0):\n",
    "        self.word_counts = {}\n",
    "        self.n_corpus_words = 0\n",
    "        for text in tqdm(corpus):\n",
    "            if isinstance(text, str):\n",
    "                text = text.split()\n",
    "            self.n_corpus_words += len(text)\n",
    "            for word in text:\n",
    "                if word not in self.word_counts:\n",
    "                    self.word_counts[word] = 0\n",
    "                self.word_counts[word] += 1\n",
    "        vocab = list(self.word_counts.keys())\n",
    "        if border < 1.0:\n",
    "            abs_border = np.quantile(list(self.word_counts.values()), border)\n",
    "        else:\n",
    "            abs_border = round(border)\n",
    "        for word in vocab:\n",
    "            if self.word_counts[word] < abs_border:\n",
    "                self.word_counts.pop(word)\n",
    "        vocab = list(self.word_counts.keys())\n",
    "        self.prefix_tree = PrefixTree(vocab)\n",
    "        print('vocab size:', len(vocab))\n",
    "        print('WordCompletor.__init__ finished')\n",
    "\n",
    "    def get_words_and_probs(\n",
    "        self, prefix: str, max_words: Union[int, None] = None\n",
    "    ) -> Tuple[List[str], List[float]]:\n",
    "        words = self.prefix_tree.search_prefix(prefix)\n",
    "        probs = [self.word_counts[word] / self.n_corpus_words for word in words]\n",
    "\n",
    "        probs_argsort = np.asarray(probs).argsort()[::-1]\n",
    "        if max_words is not None:\n",
    "            probs_argsort = probs_argsort[:max_words]\n",
    "\n",
    "        words = (np.asarray(words, dtype=\"str\")[probs_argsort]).tolist()\n",
    "        probs = (np.asarray(probs)[probs_argsort]).tolist()\n",
    "\n",
    "        return words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8c24a3e6be4b7b9dad919509ea1039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 8\n",
      "WordCompletor.__init__ finished\n"
     ]
    }
   ],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "words, probs = word_completor.get_words_and_probs(\"a\")\n",
    "words_probs = list(zip(words, probs))\n",
    "assert set(words_probs) == {\n",
    "    (\"aa\", 0.2),\n",
    "    (\"ab\", 0.2),\n",
    "    (\"aaa\", 0.1),\n",
    "    (\"abab\", 0.1),\n",
    "    (\"abb\", 0.1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание следующих слов\n",
    "\n",
    "Теперь, когда мы умеем дописывать слово за пользователем, мы можем пойти дальше и предожить ему следующее слово (или несколько) с учетом дописанного. Для этого мы воспользуемся n-граммной моделью.\n",
    "\n",
    "Напомним, что вероятность последовательности для такой модели записывается по формуле\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = \\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_{i-n}).\n",
    "$$\n",
    "\n",
    "$P(w_i \\mid w_{i-1}, \\dots, w_{i-n})$ оценивается по частоте встречаемости n-граммы.   \n",
    "\n",
    "__Задание 4 (2 балла).__ Напишите класс для n-граммной модели. Никакого сглаживания добавлять не надо, мы же не хотим, чтобы модель советовала случайные слова (хоть и очень редко)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, n):\n",
    "        # your code here\n",
    "        self.n = n\n",
    "        self.ngram_counts = {}\n",
    "        self.context_counts = {}\n",
    "\n",
    "        for text in tqdm(corpus):\n",
    "            if isinstance(text, str):\n",
    "                text = text.split()\n",
    "            for i in range(len(text) - n):\n",
    "                context = tuple(text[i: i + n])\n",
    "                next_word = text[i + n]\n",
    "                context_extended = context + (next_word,)\n",
    "\n",
    "                if context_extended not in self.ngram_counts:\n",
    "                    self.ngram_counts[context_extended] = 0\n",
    "                self.ngram_counts[context_extended] += 1\n",
    "\n",
    "                if context not in self.context_counts:\n",
    "                    self.context_counts[context] = 0\n",
    "                self.context_counts[context] += 1\n",
    "\n",
    "    def get_next_words_and_probs(self, prefix: list, max_words: Union[int, None] = None) -> Tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Возвращает список слов, которые могут идти после prefix,\n",
    "        а так же список вероятностей этих слов.\n",
    "        Возвращает не более max_words слов.\n",
    "        \"\"\"\n",
    "\n",
    "        next_words, probs = [], []\n",
    "\n",
    "        # your code here\n",
    "\n",
    "        if len(prefix) < self.n:\n",
    "            return next_words, probs\n",
    "\n",
    "        prefix = tuple(prefix[-self.n:])\n",
    "\n",
    "        if prefix not in self.context_counts:\n",
    "            return next_words, probs\n",
    "\n",
    "        for context in self.ngram_counts:\n",
    "            if context[: self.n] == prefix:\n",
    "                next_word = context[self.n]\n",
    "                next_words.append(next_word)\n",
    "                probs.append(self.ngram_counts[context] / self.context_counts[prefix])\n",
    "\n",
    "        probs_argsort = np.asarray(probs).argsort()[::-1]\n",
    "        if max_words is not None:\n",
    "            probs_argsort = probs_argsort[:max_words]\n",
    "\n",
    "        next_words = (np.asarray(next_words, dtype='str')[probs_argsort]).tolist()\n",
    "        probs = (np.asarray(probs)[probs_argsort]).tolist()\n",
    "\n",
    "        return next_words, probs\n",
    "\n",
    "    def get_next_seqs_and_probs(self,\n",
    "                                prefix: list,\n",
    "                                seq_len: int = 1,\n",
    "                                max_seqs: Union[int, None] = None,\n",
    "                                max_best_words: Union[int, None] = None) -> Tuple[List[List[str]], List[float]]:\n",
    "        \"\"\"\n",
    "        Возвращает список продолжений для prefix, продолжение состоит из seq_len слов.\n",
    "        Возвращает не более max_seqs вариантов продолжений.\n",
    "        \"\"\"\n",
    "        next_seqs, probs = [], []\n",
    "        next_words, word_probs = self.get_next_words_and_probs(prefix, max_best_words)\n",
    "\n",
    "        if seq_len == 1:\n",
    "            return [[word] for word in next_words], word_probs\n",
    "\n",
    "        for (word, word_prob) in zip(next_words, word_probs):\n",
    "            local_seqs, local_probs = self.get_next_seqs_and_probs(prefix + [word], seq_len - 1, max_seqs,\n",
    "                                                                   max_best_words)\n",
    "            local_seqs = [[word] + local_seq for local_seq in local_seqs]\n",
    "            local_probs = [word_prob * local_prob for local_prob in local_probs]\n",
    "\n",
    "            next_seqs.extend(local_seqs)\n",
    "            probs.extend(local_probs)\n",
    "\n",
    "        probs_argsort = np.asarray(probs).argsort()[::-1]\n",
    "        if max_seqs is not None:\n",
    "            probs_argsort = probs_argsort[:max_seqs]\n",
    "\n",
    "        next_seqs = (np.asarray(next_seqs, dtype='str')[probs_argsort]).tolist()\n",
    "        probs = (np.asarray(probs)[probs_argsort]).tolist()\n",
    "\n",
    "        return next_seqs, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4979a70583d49ebbaa4a3c48a3ce2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"aa\", \"aa\", \"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "\n",
    "next_words, probs = n_gram_model.get_next_words_and_probs([\"aa\", \"aa\"])\n",
    "words_probs = list(zip(next_words, probs))\n",
    "\n",
    "assert set(words_probs) == {(\"aa\", 2 / 3), (\"ab\", 1 / 3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы теперь можем объединить два метода в автоматический дописыватель текстов: первый будет дополнять слово, а второй – предлагать продолжения. Хочется, чтобы предлагался список возможных продолжений, из который пользователь сможет выбрать наиболее подходящее. Самое сложное тут – аккуратно выбирать, что показывать, а что нет.   \n",
    "\n",
    "__Задание 5 (2 балла).__ В качестве первого подхода к снаряду реализуйте метод, возвращающий всегда самое вероятное продолжение жадным способом. После этого можно добавить опцию генерации нескольких вариантов продолжений, что сделает метод гораздо лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class TextSuggestion:\n",
    "    def __init__(self, word_completor: WordCompletor, n_gram_model: NGramLanguageModel):\n",
    "        self.word_completor = word_completor\n",
    "        self.n_gram_model = n_gram_model\n",
    "\n",
    "    def suggest_text(\n",
    "        self,\n",
    "        text: Union[str, list],\n",
    "        n_words: int = 3,\n",
    "        n_texts: int = 1,\n",
    "        max_complete_words: Union[int, None] = 1,\n",
    "        n_best_words: int = 1,\n",
    "        return_probs: bool = False,\n",
    "    ) -> (List[List[str]], Optional[List[float]]): # type: ignore\n",
    "        \"\"\"\n",
    "        Возвращает возможные варианты продолжения текста (по умолчанию только один)\n",
    "\n",
    "        text: строка или список слов – написанный пользователем текст\n",
    "        n_words: число слов, которые дописывает n-граммная модель\n",
    "        n_texts: число возвращаемых продолжений\n",
    "        max_complete_words: число вариантов дополнения последнего слова\n",
    "        n_best_words: число вариантов, которые n-граммная модель предлагает для каждого слова\n",
    "\n",
    "        return: list[list[srt]] – список из n_texts списков слов, по 1 + n_words слов в каждом\n",
    "        Первое слово – это то, которое WordCompletor дополнил до целого.\n",
    "        \"\"\"\n",
    "\n",
    "        # your code here\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            text = text.split()\n",
    "\n",
    "        # дописываем последнее слово\n",
    "        complete_words, complete_probs = self.word_completor.get_words_and_probs(\n",
    "            text[-1], max_complete_words\n",
    "        )\n",
    "\n",
    "        if not complete_words:\n",
    "            return [[text[-1]] for _ in range(n_texts)]\n",
    "\n",
    "        # для каждого варианта дополнения слова напишем продолжение\n",
    "        n_sugg_per_completed = int(max(3, np.ceil(2 * n_texts / len(complete_words))))\n",
    "        suggestions = []\n",
    "        suggestions_probs = []\n",
    "\n",
    "        for ind_completed, completed_word in enumerate(complete_words):\n",
    "            history = text[:-1] + [completed_word]\n",
    "            local_sugg, local_sugg_probs = self.n_gram_model.get_next_seqs_and_probs(\n",
    "                history, n_words, n_sugg_per_completed, n_best_words\n",
    "            )\n",
    "            local_sugg = [[completed_word] + seq for seq in local_sugg]\n",
    "            local_sugg_probs = [local_prob *  complete_probs[ind_completed] for local_prob in local_sugg_probs]\n",
    "\n",
    "            suggestions.extend(local_sugg)\n",
    "            suggestions_probs.extend(local_sugg_probs)\n",
    "\n",
    "        probs_argsort = np.asarray(suggestions_probs).argsort()[::-1]\n",
    "        probs_argsort = probs_argsort[:n_texts]\n",
    "\n",
    "        suggestions = (np.asarray(suggestions, dtype=\"str\")[probs_argsort]).tolist()\n",
    "        suggestions_probs = (np.asarray(suggestions_probs)[probs_argsort]).tolist()\n",
    "\n",
    "        if return_probs:\n",
    "            return suggestions, suggestions_probs\n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b999a1962640208bb5af340f2412e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 8\n",
      "WordCompletor.__init__ finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3caa545baf1d4c1eb22154073abec158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"aa\", \"aa\", \"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)\n",
    "\n",
    "assert text_suggestion.suggest_text([\"aa\", \"aa\"], n_words=3, n_texts=1) == [\n",
    "    [\"aa\", \"aa\", \"aa\", \"aa\"]\n",
    "]\n",
    "assert text_suggestion.suggest_text([\"abb\", \"aa\", \"ab\"], n_words=2, n_texts=1) == [\n",
    "    [\"ab\", \"bba\", \"bbb\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть: Добавляем UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускать ячейки в юпитере – это хорошо, но будет лучше, если вашим решением действительно можно будет пользоваться. Для этого вам предлагается добавить полноценных User Interface. Мы рекомендуем использовать для этого [reflex](https://github.com/reflex-dev/reflex). Это Python библиотека для создания web-интерфейсом с очень богатым функционалом.\n",
    "\n",
    "Ваша задача – сделать поле для текстового ввода, при наборе текста в котором будут появляться подсказки в реальном времени. Продумайте, как пользователь будет выбирать подсказки, сколько продолжений рекомендавать и так далее. В общем, должно получиться красиво и удобно. В этой части вы можете модифицировать все классы по своему усмотрению и добавлять любые эвристики. Если нужно, то дополнительно обрабатывать текст и вообще делать все, что считаете нужным. \n",
    "\n",
    "За это задание можно получить до __5-ти бонусных баллов__ в зависимости о того, насколько хорошо и удобно у вас получилось. При сдаче задания прикрепите небольшой __отчет__ (полстраницы) с описанием вашей системы, а также __видео__ (1-2 минуты) с демонстрацией работы интерфейса.\n",
    "\n",
    "Мы настоятельно рекомендуем вам оформить код в проект, а не писать в ноутбуке. Но если вам очень хочется писать тут, то хотя бы не меняйте код в предыдущих заданиях, чтобы его можно было нормально оценивать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Сначала проверим работоспособность метода, без интерфейса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed: 19.7min remaining: 59.2min\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed: 21.0min remaining: 35.0min\n",
      "[Parallel(n_jobs=8)]: Done   4 out of   8 | elapsed: 21.0min remaining: 21.0min\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed: 21.3min remaining: 12.8min\n",
      "[Parallel(n_jobs=8)]: Done   6 out of   8 | elapsed: 23.8min remaining:  7.9min\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed: 24.9min finished\n"
     ]
    }
   ],
   "source": [
    "N_CORES = cpu_count()\n",
    "\n",
    "list_array = np.array_split(emails[\"message\"], N_CORES)\n",
    "clear_texts = Parallel(n_jobs=N_CORES, verbose=10)(\n",
    "    delayed(lambda array: list(map(clear_text, array)))(array)\n",
    "    for array in np.array_split(emails[\"message\"], N_CORES)\n",
    ")\n",
    "clear_texts = [x for x in sum(clear_texts, []) if len(x) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "corpus_border = 5\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f332eb1cd64d158dbbfa4078582144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 113296\n",
      "WordCompletor.__init__ finished\n"
     ]
    }
   ],
   "source": [
    "word_completor = WordCompletor(clear_texts, corpus_border)\n",
    "\n",
    "# save\n",
    "with open(f\"word_completor.pickle\", \"wb\") as file:\n",
    "    pickle.dump(word_completor, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4b46496d484762bd412b1c273f69d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_gram_model = NGramLanguageModel(corpus=clear_texts, n=n)\n",
    "\n",
    "# save\n",
    "with open(f\"n_gram_model.pickle\", \"wb\") as file:\n",
    "    pickle.dump(n_gram_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load them\n",
    "with open(f\"word_completor.pickle\", \"rb\") as file:\n",
    "    word_completor = pickle.load(file)\n",
    "with open(f\"n_gram_model.pickle\", \"rb\") as file:\n",
    "    n_gram_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)\n",
    "# save\n",
    "with open(f\"text_suggestion.pickle\", \"wb\") as file:\n",
    "    pickle.dump(text_suggestion, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['pond', 'requirement', 'used', 'data'],\n",
       "  ['pond', 'street', 'widening', 'even']],\n",
       " [2.2107073808934363e-06, 2.2107073808934363e-06])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_suggestion.suggest_text('retention filtration pon'.split(), n_texts=5, max_complete_words=3, n_best_words=3, return_probs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> отдельный GitHub для бонуса: https://github.com/GrishHelen/DL_NLP_TextSugg.git\n",
    "> \n",
    "> Отчет я положила в README.md, демонстрация лежит отдельным файлом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
